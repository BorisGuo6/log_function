# Logarithmic Function Matters Policy Gradient Deep Reinforcement Learning
This paper studies the influence of logarithmic functions in policy gradient deep reinforcement learning (RL) methods, a subject previously under-emphasized in the field. It delves into how different logarithmic bases, specifically loge, log2, and log10, influence the efficiency of policy gradient methods. We analyze the role of these logarithmic functions and their gradients when using Stochastic Gradient Descent as the optimizer in deep RL. Experimental results show that the choice of the logarithmic base influences the policy gradient methods. loge and log2 are generally more effective than log10 in deep RL algo- rithms, but log10 exhibits more stability. Based on analysis and experimental results, we propose the Logarithmic Basis Policy Gradient (LBPG) and Adaptive LBPG algorithms to enhance policy gradient methods. LBPG denotes policy gradient methods with loge, log2, and log10 as the logarithmic base, respectively. Furthermore, Adaptive LBPG dynamically selects logarithmic basis among loge, log2, and log10 based on the variance-to-mean ratio of returns, to optimize speed and stability. Experimental results show the importance of choosing appropriate logarithmic functions for policy gradient RL methods and highlight Adaptive LBPGâ€™s potential to refine learning in deep RL tasks.
